# Alphabet Soup Charity: Predicting the Success of Funded Projects
*Comprehensive Deep Learning Assignment for Data Science Bootcamp, Module 21*

## Background & Objectives
*The nonprofit foundation Alphabet Soup wants a tool that can help it select the applicants for funding with the best chance of success in their ventures. With your knowledge of machine learning and neural networks, youâ€™ll use the features in the provided dataset to create a binary classifier that can predict whether applicants will be successful if funded by Alphabet Soup.*

The primary objective of this assignment is to construct a deep learning model capable of predicting the success of projects funded by the Alphabet Soup Charity. In a philanthropic landscape where every dollar counts, the organization seeks to optimize its funding allocation to ensure maximum impact. Utilizing advanced neural network models, this project aims to provide actionable insights into various contributing factors that indicate project success. The end goal is for the charity to implement this model as a robust, data-driven tool in their funding decision-making process.

## Dependencies
To run the code and notebooks, the following Python libraries are required:

- **Pandas**: For data manipulation and exploration
- **Scikit-Learn**: For traditional machine learning tasks and evaluation metrics
- **TensorFlow**: For constructing and evaluating deep learning models

### Installation Instructions
To install these dependencies, run the following command in your terminal:
```bash
pip install pandas scikit-learn tensorflow
```

## Assignment Files
The repository is structured to include the following pivotal files:

- `Charity Notebook.ipynb`: The Jupyter Notebook that contains the first iteration of the entire analysis, from data preprocessing to model evaluation.
- `Optimization Notebook.ipynb`: A separate Jupyter Notebook that provides a more in-depth look into the optimization techniques applied to the initial model (includes three separate models).
- `Model Report.md`: A comprehensive markdown file that reports on the results, performance metrics, and findings from both the initial and optimized deep learning models.
- `H5 Models`: A directory that contains the *H5* files for all the models used in the analysis.

## Analysis Steps
The project is meticulously structured into several core stages:

1. **Data Preprocessing**: Handling missing values, encoding categorical variables, and data scaling.
2. **Compiling, Training, and Evaluating the Initial Model**: Includes layer architecture, activation functions, and initial model performance.
3. **Model Optimization**: Iterative steps to improve model accuracy beyond initial benchmarks, using additional layers, neurons, and activation functions.
4. **Final Model Evaluation**: Detailed assessment of the final optimized model, including metrics like accuracy and loss.
5. **Report Generation**: Compilation of all findings into a digestible format.

*Each stage is elaborated upon in the accompanying Jupyter Notebooks. The code has been developed and tested in Visual Studio Code using Python 3.11.*
## Coding Soundtrack
The coding soundtrack for this week's challenge was Memphis May Fire's album, [Remade in Misery](https://www.youtube.com/playlist?list=PLxA687tYuMWiLo0Us2Iv-nGWjIdCecdM7).

## Further Reading
For a more detailed understanding of the model's performance and the methodology used for optimization, please refer to `Model Report.md`.

*-LM95A1*